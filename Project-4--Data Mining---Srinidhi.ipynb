{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA MINING- Clustering, CART, Random Forest & Artificial Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from scipy.stats import zscore\n",
    "from kneed import KneeLocator\n",
    "from sklearn.cluster import KMeans \n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Clustering\n",
    "### A leading bank wants to develop a customer segmentation to give promotional offers to its customers. They collected a sample that summarizes the activities of users during the past few months. You are given the task to identify the segments based on credit card usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkt = pd.read_csv('bank_marketing_part1_Data-1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1  Read the data and do exploratory data analysis. Describe the data briefly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mkt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkt.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mkt.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Summary- Measures of Central Tendency & Measures of Dispersion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkt.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"data:\",mkt.mode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mkt.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mkt.max() - mkt.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkt.quantile(0.75) - mkt.quantile(0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cv = mkt.std()/mkt.mean()\n",
    "print(cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shape of the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkt.skew() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Five Number Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkt.boxplot(column=\"spending\",figsize=(6,6))\n",
    "\n",
    "plt.text(x=0.74, y=17.3, s=\"3rd Quartile\")\n",
    "plt.text(x=0.8, y=14.3, s=\"Median\")\n",
    "plt.text(x=0.75, y=12.2, s=\"1st Quartile\")\n",
    "plt.text(x=0.9, y=10.5, s=\"Min\")\n",
    "plt.text(x=0.9, y=21.1, s=\"Max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkt.boxplot(column=\"advance_payments\",figsize=(6,6))\n",
    "\n",
    "plt.text(x=0.74, y=15.7, s=\"3rd Quartile\")\n",
    "plt.text(x=0.8, y=14.3, s=\"Median\")\n",
    "plt.text(x=0.75, y=13.4, s=\"1st Quartile\")\n",
    "plt.text(x=0.9, y=12.4, s=\"Min\")\n",
    "plt.text(x=0.9, y=17.2, s=\"Max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkt.boxplot(column=\"probability_of_full_payment\",figsize=(6,6))\n",
    "\n",
    "plt.text(x=0.74, y=0.887, s=\"3rd Quartile\")\n",
    "plt.text(x=0.8, y=0.873, s=\"Median\")\n",
    "plt.text(x=0.75, y=0.856, s=\"1st Quartile\")\n",
    "plt.text(x=0.9, y=0.808, s=\"Min\")\n",
    "plt.text(x=0.9, y=0.918, s=\"Max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkt.boxplot(column=\"current_balance\",figsize=(6,6))\n",
    "\n",
    "plt.text(x=0.74, y=5.9, s=\"3rd Quartile\")\n",
    "plt.text(x=0.8, y=5.5, s=\"Median\")\n",
    "plt.text(x=0.75, y=5.2, s=\"1st Quartile\")\n",
    "plt.text(x=0.9, y=4.8, s=\"Min\")\n",
    "plt.text(x=0.9, y=6.6, s=\"Max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkt.boxplot(column=\"credit_limit\",figsize=(6,6))\n",
    "\n",
    "plt.text(x=0.74, y=3.5, s=\"3rd Quartile\")\n",
    "plt.text(x=0.8, y=3.2, s=\"Median\")\n",
    "plt.text(x=0.75, y=2.9, s=\"1st Quartile\")\n",
    "plt.text(x=0.9, y=2.6, s=\"Min\")\n",
    "plt.text(x=0.9, y=4.0, s=\"Max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkt.boxplot(column=\"min_payment_amt\",figsize=(6,6))\n",
    "\n",
    "plt.text(x=0.74, y=4.7, s=\"3rd Quartile\")\n",
    "plt.text(x=0.8, y=3.5, s=\"Median\")\n",
    "plt.text(x=0.75, y=2.5, s=\"1st Quartile\")\n",
    "plt.text(x=0.9, y=0.7, s=\"Min\")\n",
    "plt.text(x=0.9, y=8.4, s=\"Max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mkt.boxplot(column=\"max_spent_in_single_shopping\",figsize=(6,6))\n",
    "\n",
    "plt.text(x=0.74, y=5.8, s=\"3rd Quartile\")\n",
    "plt.text(x=0.8, y=5.2, s=\"Median\")\n",
    "plt.text(x=0.75, y=5.0, s=\"1st Quartile\")\n",
    "plt.text(x=0.9, y=4.5, s=\"Min\")\n",
    "plt.text(x=0.9, y=6.5, s=\"Max\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Covariance & Correlation of the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkt.cov()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkt.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(7,5.5))   \n",
    "sns.heatmap(mkt.corr(), ax=ax, annot=True, linewidths=0.05, fmt= '.2f',cmap=\"magma\") # the color intensity is based on \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2  Do you think scaling is necessary for clustering in this case? Justify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * Yes, scaling is necessary. \n",
    "#### * Scaling, in general, is done so that all the variables under consideration is given the same weightage. In the given dataset, spending is in 1000's, advance payments is in 100's and credit limit is in 10000's- this implies that different variables will be given different weightages. \n",
    "#### * Clustering techniques use Distance methods like Euclidean, Manhattan etc., to compute distances between clusters which is highly affected by the unscaled variables making the models ineffective.\n",
    "#### * Hence Scaling is very important in case of Clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Apply hierarchical clustering to scaled data. Identify the number of optimum clusters using Dendrogram and briefly describe them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkt_hc = pd.read_csv('bank_marketing_part1_Data-1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = mkt_hc.apply(zscore)\n",
    "df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wardlink = linkage(df_1, method = 'ward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dend = dendrogram(wardlink)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dend = dendrogram(wardlink,truncate_mode='lastp',p = 6,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = fcluster(wardlink, 3, criterion='maxclust')\n",
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = fcluster(wardlink, 15, criterion='distance')\n",
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkt_hc['clusters'] = clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mkt_hc.to_csv('hc.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Apply K-Means clustering on scaled data and determine optimum clusters. Apply elbow curve and silhouette score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkt_km = pd.read_csv('bank_marketing_part1_Data-1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = mkt_km.apply(zscore)\n",
    "df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means = KMeans(n_clusters = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means.fit(df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means_1 = KMeans(n_clusters = 1)\n",
    "k_means_1.fit(df_2)\n",
    "k_means_1.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means_2 = KMeans(n_clusters = 2)\n",
    "k_means_2.fit(df_2)\n",
    "k_means_2.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means_3 = KMeans(n_clusters = 3)\n",
    "k_means_3.fit(df_2)\n",
    "k_means_3.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means_4 = KMeans(n_clusters = 4)\n",
    "k_means_4.fit(df_2)\n",
    "k_means_4.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means_5 = KMeans(n_clusters = 5)\n",
    "k_means_5.fit(df_2)\n",
    "k_means_5.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means_6 = KMeans(n_clusters = 6)\n",
    "k_means_6.fit(df_2)\n",
    "k_means_6.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wss =[] \n",
    "for i in range(1,11):\n",
    "    KM = KMeans(n_clusters=i)\n",
    "    KM.fit(df_2)\n",
    "    wss.append(KM.inertia_)\n",
    "print(wss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1,11), wss);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means_3 = KMeans(n_clusters = 3)\n",
    "k_means_3.fit(df_2)\n",
    "labels = k_means_3.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkt_km[\"Clus_kmeans\"] = labels\n",
    "mkt_km.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_score(df_2,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sil_width = silhouette_samples(mkt_km,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkt_km[\"sil_width\"] = sil_width\n",
    "mkt_km.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_samples(df_2,labels).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkt_km.to_csv('km.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Describe cluster profiles for the clusters defined. Recommend different promotional strategies for different clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkt_grouped_hc = mkt_hc.groupby(mkt_hc.clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkt_cluster_1_hc = mkt_grouped_hc.get_group(1)\n",
    "mkt_cluster_2_hc = mkt_grouped_hc.get_group(2)\n",
    "mkt_cluster_3_hc = mkt_grouped_hc.get_group(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkt_cluster_1_hc.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mkt_cluster_2_hc.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkt_cluster_3_hc.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkt_grouped_km = mkt_km.groupby(mkt_km.Clus_kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkt_cluster_1_km = mkt_grouped_km.get_group(0)\n",
    "mkt_cluster_2_km = mkt_grouped_km.get_group(1)\n",
    "mkt_cluster_3_km = mkt_grouped_km.get_group(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkt_cluster_1_km.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkt_cluster_2_km.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mkt_cluster_3_km.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "•\tCluster 1 in Hierarchical Clustering is mapped to Cluster 0 in K-Means.\n",
    "•\tCluster 2 in Hierarchical Clustering is mapped to Cluster 2 in K-Means.\n",
    "•\tCluster 3 in Hierarchical Clustering is mapped to Cluster 1 in K-Means.\n",
    "\n",
    "A.\tPromotional Strategies for Cluster 1 in Hierarchical Clustering & Cluster 0 in K-Means: \n",
    "\n",
    "This cluster is the richer category observing their ‘spending’, ‘advance payments’, ‘credit limit’, ‘minimum payment amount’ and ‘maximum spent in single shopping’ which is the highest amongst the others.\n",
    "To convert them into more profitable customers:\n",
    "\n",
    "i.\tIncrease their credit limit, so that they use it more frequently as the chance of them being converted into bad customers are very less.\n",
    "\n",
    "ii.\tSince they are the highest spenders of the three clusters, they can be offered a few loyalty rewards to hold them back as our customers. This may include a round-trip to various countries, cashback/reward points to the highest spender(s) of the week.\n",
    "\n",
    "\n",
    "B.\tPromotional Strategies for Cluster 2 in Hierarchical Clustering & Cluster 2 in K-Means: \n",
    "\n",
    "This cluster comes under the ‘aspiring spenders’ (middle income) category as almost all the parameters under consideration are closer to the previous category. \n",
    "\n",
    "Since their aspirations to spend is high, the chances that they will churn out is also high. To hold them as a loyal customers, instead of increasing their credit limit (as this can prove to be a risky category of customer as well), the bank should offer more attractive promotional strategy to this group than the remaining group of customers. \n",
    "\n",
    "\n",
    "C.\tPromotional Strategies for Cluster 3 in Hierarchical Clustering & Cluster 1 in K-Means: \n",
    "\n",
    "This is the category consisting of low-income people with lowest spending and advance payments. However, the minimum payment amount is the highest.\n",
    "\n",
    "The best promotional strategies for this group would be:\n",
    "\n",
    "i.\tProviding an EMI option on purchases (say, up to maximum of 6 months) which might sound attractive to them and they can easily purchase now and pay later.\n",
    "This is also one of the ways to generate some interest to the banks from this category.\n",
    "\n",
    "ii.\tInstead of providing cashbacks/other promotional offers, the best suited promotional strategies would be to provide product discounts (say 2% to 5% on particular essential products). This way, the chances that these customer will churn out would reduce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: CART-RF-ANN\n",
    "### An Insurance firm providing tour insurance is facing higher claim frequency. The management decides to collect data from the past few years. You are assigned the task to make a model which predicts the claim status and provide recommendations to management. Use CART, RF & ANN and compare the models' performances in train and test sets. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins = pd.read_csv('insurance_part2_data-2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1  Data Ingestion: Read the dataset. Do the descriptive statistics and do null value condition check, write an inference on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Summary- Measures of Central Tendency & Measures of Dispersion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ins.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"data:\",ins.mode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ins.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Q1 = ins.quantile(0.25)\n",
    "Q3 = ins.quantile(0.75)\n",
    "IQR = Q3 - Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = ins.std()/ins.mean()\n",
    "print(cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shape of the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins.skew() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Five Number Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ins.boxplot(column=\"Age\",figsize=(6,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins.boxplot(column=\"Commision\",figsize=(6,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins.boxplot(column=\"Duration\",figsize=(6,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins.boxplot(column=\"Sales\",figsize=(6,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Covariance & Correlation of the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins.cov()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(7,5.5))   \n",
    "sns.heatmap(ins.corr(), ax=ax, annot=True, linewidths=0.05, fmt= '.2f',cmap=\"magma\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking for Missing Values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking for Duplicate Rows & Dropping them, if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dups = ins.duplicated()\n",
    "print('Number of duplicate rows = %d' % (dups.sum()))\n",
    "ins[dups].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins = ins.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins.loc[ins['Duration'] > 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins['Duration'].replace(0,ins['Duration'].median(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins['Duration'].replace(-1,ins['Duration'].median(),inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Values Counts for the Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Agency_code \\n',ins.Agency_Code.value_counts())\n",
    "print('\\n')\n",
    "print('Type \\n',ins.Type.value_counts())\n",
    "print('\\n')\n",
    "print('Claimed \\n',ins.Claimed.value_counts())\n",
    "print('\\n')\n",
    "print('Channel \\n',ins.Channel.value_counts())\n",
    "print('\\n')\n",
    "print('Product Name \\n',ins.Product_Name.value_counts())\n",
    "print('\\n')\n",
    "print('Destination \\n',ins.Destination.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting the Categorical to Numerical to facilitate the Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for feature in ins.columns: \n",
    "    if ins[feature].dtype == 'object': \n",
    "        print('\\n')\n",
    "        print('feature:',feature)\n",
    "        print(pd.Categorical(ins[feature].unique()))\n",
    "        print(pd.Categorical(ins[feature].unique()).codes)\n",
    "        ins[feature] = pd.Categorical(ins[feature]).codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Treating the Outliers- IQR Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outlier(col):\n",
    "    sorted(col)\n",
    "    Q1,Q3=np.percentile(col,[25,75])\n",
    "    IQR=Q3-Q1\n",
    "    lower_range= Q1-(1.5 * IQR)\n",
    "    upper_range= Q3+(1.5 * IQR)\n",
    "    return lower_range, upper_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lratio,uratio=remove_outlier(ins['Age'])\n",
    "ins['Age']=np.where(ins['Age']>uratio,uratio,ins['Age'])\n",
    "ins['Age']=np.where(ins['Age']<lratio,lratio,ins['Age'])\n",
    "\n",
    "lraxis,uraxis=remove_outlier(ins['Commision'])\n",
    "ins['Commision']=np.where(ins['Commision']>uraxis,uraxis,ins['Commision'])\n",
    "ins['Commision']=np.where(ins['Commision']<lraxis,lraxis,ins['Commision'])\n",
    "\n",
    "lraspect,uraspect=remove_outlier(ins['Duration'])\n",
    "ins['Duration']=np.where(ins['Duration']>uraspect,uraspect,ins['Duration'])\n",
    "ins['Duration']=np.where(ins['Duration']<lraspect,lraspect,ins['Duration'])\n",
    "\n",
    "lrscaled_var,urscaled_var=remove_outlier(ins['Sales'])\n",
    "ins['Sales']=np.where(ins['Sales']>urscaled_var,urscaled_var,ins['Sales'])\n",
    "ins['Sales']=np.where(ins['Sales']<lrscaled_var,lrscaled_var,ins['Sales'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins.boxplot(column=\"Sales\",figsize=(6,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins.boxplot(column=\"Age\",figsize=(6,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins.boxplot(column=\"Commision\",figsize=(6,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins.boxplot(column=\"Duration\",figsize=(6,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2  Data Split: Split the data into test and train, build classification model CART, Random Forest, Artificial Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the Independent & Dependent Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ins.drop(\"Claimed\", axis=1)\n",
    "y = ins.pop(\"Claimed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the data into Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.30, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X_train',X_train.shape)\n",
    "print('X_test',X_test.shape)\n",
    "print('train_labels',y_train.shape)\n",
    "print('test_labels',y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DECISION TREE CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dt = DecisionTreeClassifier(criterion = 'gini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dt.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "train_char_label = ['No', 'Yes']\n",
    "Insurance_Tree_File = open('Insurance_Tree_File.dot','w')\n",
    "dot_data = tree.export_graphviz(model_dt, \n",
    "                                out_file=Insurance_Tree_File, \n",
    "                                feature_names = list(X_train), \n",
    "                                class_names = list(train_char_label))\n",
    "\n",
    "Insurance_Tree_File.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame(model_dt.feature_importances_, columns = [\"Imp\"], index = X_train.columns).sort_values('Imp',ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = model_dt.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_dt = {\n",
    "    'max_depth': [4,6,7],\n",
    "    'min_samples_leaf': [30,35,40],\n",
    "    'min_samples_split': [60,80,120]\n",
    "}\n",
    "\n",
    "model_dt = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "grid_search_dt = GridSearchCV(estimator = model_dt, param_grid = param_grid_dt, cv = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_dt.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dt_reg = DecisionTreeClassifier(criterion = 'gini', max_depth = 4, min_samples_leaf=30, min_samples_split=120, random_state=1)\n",
    "model_dt_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "train_char_label = ['No', 'Yes']\n",
    "Insurance_Tree_File_reg = open('Insurance_Tree_File_reg.dot','w')\n",
    "dot_data = tree.export_graphviz(model_dt_reg, \n",
    "                                out_file=Insurance_Tree_File_reg, \n",
    "                                feature_names = list(X_train), \n",
    "                                class_names = list(train_char_label))\n",
    "\n",
    "Insurance_Tree_File_reg.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (pd.DataFrame(model_dt_reg.feature_importances_, columns = [\"Imp\"], index = X_train.columns).sort_values('Imp',ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_predict_dt = model_dt_reg.predict(X_train)\n",
    "y_test_predict_dt = model_dt_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_predict_dt_prob_a = model_dt_reg.predict_proba(X_train)\n",
    "y_test_predict_dt_prob_a = model_dt_reg.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dt_reg.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dt_reg.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RANDOM FOREST CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_rf = {\n",
    "    'max_depth': [6,7,8],\n",
    "    'max_features': [5,6,7,8],\n",
    "    'min_samples_leaf': [20,30,40],\n",
    "    'min_samples_split': [40,45,50],\n",
    "    'n_estimators': [100,200,300]\n",
    "}\n",
    "\n",
    "rfcl = RandomForestClassifier(random_state = 1)\n",
    "rfcl.fit(X_train, y_train)\n",
    "\n",
    "grid_search_rf = GridSearchCV(estimator = rfcl, param_grid = param_grid_rf, cv = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_rf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_grid_rf = grid_search_rf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf_reg = RandomForestClassifier(n_estimators = 201,max_depth = 7,max_features = 7,min_samples_leaf = 20 , min_samples_split = 45 ,oob_score = True,random_state = 1)\n",
    "model_rf_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train_predict_rf = model_rf_reg.predict(X_train)\n",
    "y_test_predict_rf = model_rf_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_predict_rf_prob_a = model_rf_reg.predict_proba(X_train)\n",
    "y_test_predict_rf_prob_a = model_rf_reg.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_rf_reg.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf_reg.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf_reg.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1-0.7667332667332667"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (pd.DataFrame(model_rf_reg.feature_importances_, columns = [\"Imp\"], index = X_train.columns).sort_values('Imp',ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP CLASSIFIER (ARTIFICIAL NEURAL NETWORK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A target variable with a large spread of values, in turn, may result in large error gradient values causing weight values to change dramatically, making the learning process unstable. Scaling input and output variables is a critical step in using neural network models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = StandardScaler() \n",
    "X_train_z = z.fit_transform(X_train) \n",
    "X_test_z = z.transform (X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_mlp = {\n",
    "    'hidden_layer_sizes': [(100,100,100)],\n",
    "    'activation': ['logistic', 'relu'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'tol': [0.1,0.01,0.001],\n",
    "    'max_iter' : [150,300]\n",
    "}\n",
    "\n",
    "nncl = MLPClassifier(random_state=1)\n",
    "\n",
    "grid_search_mlp = GridSearchCV(estimator = nncl, param_grid = param_grid_mlp, cv = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_mlp.fit(X_train_z, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_mlp.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_grid_mlp = grid_search_mlp.best_estimator_\n",
    "best_grid_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes=(100, 100, 100), activation ='relu',max_iter = 150,solver = 'adam',verbose = True,random_state = 1, tol = 0.001 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train_z, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_predict_mlp = clf.predict(X_train_z)\n",
    "y_test_predict_mlp = clf.predict(X_test_z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3  Performance Metrics: Check the performance of Predictions on Train and Test sets using Accuracy, Confusion Matrix, Plot ROC curve and get ROC_AUC score for each model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A .  DECISION TREE CLASSIFIER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A.1  Decision Tree Classifier - TRAIN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "confusion_matrix(y_train, y_train_predict_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_train, y_train_predict_dt)\n",
    "sns.heatmap(confusion_matrix(y_train, y_train_predict_dt),annot=True, fmt='d',cbar=False, cmap=\"mako\")\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('Actual Label')\n",
    "plt.title('Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Data Accuracy\n",
    "cart_train_acc=model_rf_reg.score(X_train,y_train) \n",
    "cart_train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_train,y_train_predict_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cart_metrics=classification_report(y_train, y_train_predict_dt,output_dict=True)\n",
    "df=pd.DataFrame(cart_metrics).transpose()\n",
    "cart_train_f1=round(df.loc[\"1\"][2],2)\n",
    "cart_train_recall=round(df.loc[\"1\"][1],2)\n",
    "cart_train_precision=round(df.loc[\"1\"][0],2)\n",
    "print ('cart_train_precision ',cart_train_precision)\n",
    "print ('cart_train_recall ',cart_train_recall)\n",
    "print ('cart_train_f1 ',cart_train_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict probabilities\n",
    "probs = model_dt_reg.predict_proba(X_train)\n",
    "\n",
    "# keep probabilities for the positive outcome only\n",
    "probs = probs[:, 1]\n",
    "\n",
    "# calculate AUC\n",
    "cart_train_auc = roc_auc_score(y_train, probs)\n",
    "print('AUC: %.3f' % cart_train_auc)\n",
    "\n",
    "# calculate roc curve\n",
    "cart_train_fpr, cart_train_tpr, cart_trainthresholds = roc_curve(y_train, probs)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "\n",
    "# plot the roc curve for the model\n",
    "plt.plot(cart_train_fpr, cart_train_tpr, marker='.')\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A.2  Decision Tree Classifier - TEST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_test_predict_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_test_predict_dt)\n",
    "sns.heatmap(confusion_matrix(y_test, y_test_predict_dt),annot=True, fmt='d',cbar=False, cmap=\"mako\")\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('Actual Label')\n",
    "plt.title('Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Data Accuracy\n",
    "cart_test_acc=model_rf_reg.score(X_test,y_test)\n",
    "cart_test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_test_predict_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cart_metrics=classification_report(y_test,y_test_predict_dt,output_dict=True)\n",
    "df=pd.DataFrame(cart_metrics).transpose()\n",
    "cart_test_precision=round(df.loc[\"1\"][0],2)\n",
    "cart_test_recall=round(df.loc[\"1\"][1],2)\n",
    "cart_test_f1=round(df.loc[\"1\"][2],2)\n",
    "print ('cart_test_precision ',cart_test_precision)\n",
    "print ('cart_test_recall ',cart_test_recall)\n",
    "print ('cart_test_f1 ',cart_test_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict probabilities\n",
    "probs = model_dt_reg.predict_proba(X_test)\n",
    "\n",
    "# keep probabilities for the positive outcome only\n",
    "probs = probs[:, 1]\n",
    "\n",
    "# calculate AUC\n",
    "cart_test_auc = roc_auc_score(y_test, probs)\n",
    "print('AUC: %.3f' % cart_test_auc)\n",
    "\n",
    "# calculate roc curve\n",
    "cart_test_fpr, cart_test_tpr, cart_testthresholds = roc_curve(y_test, probs)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "\n",
    "# plot the roc curve for the model\n",
    "plt.plot(cart_test_fpr, cart_test_tpr, marker='.')\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B.  RANDOM FOREST CLASSIFIER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B.1  Random Forest Classifier - TRAIN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "confusion_matrix(y_train, y_train_predict_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_train, y_train_predict_rf)\n",
    "sns.heatmap(confusion_matrix(y_train, y_train_predict_rf),annot=True, fmt='d',cbar=False, cmap=\"mako\")\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('Actual Label')\n",
    "plt.title('Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_train_acc=model_rf_reg.score(X_train,y_train) \n",
    "rf_train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train,y_train_predict_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_metrics=classification_report(y_train,y_train_predict_rf,output_dict=True)\n",
    "df=pd.DataFrame(rf_metrics).transpose()\n",
    "rf_train_precision=round(df.loc[\"1\"][0],2)\n",
    "rf_train_recall=round(df.loc[\"1\"][1],2)\n",
    "rf_train_f1=round(df.loc[\"1\"][2],2)\n",
    "print ('rf_train_precision ',rf_train_precision)\n",
    "print ('rf_train_recall ',rf_train_recall)\n",
    "print ('rf_train_f1 ',rf_train_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict probabilities\n",
    "probs = model_rf_reg.predict_proba(X_train)\n",
    "\n",
    "# keep probabilities for the positive outcome only\n",
    "probs = probs[:, 1]\n",
    "\n",
    "# calculate AUC\n",
    "rf_train_auc = roc_auc_score(y_train, probs)\n",
    "print('AUC: %.3f' % rf_train_auc)\n",
    "\n",
    "# calculate roc curve\n",
    "rf_train_fpr,rf_train_tpr, thresholds = roc_curve(y_train, probs)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "\n",
    "# plot the roc curve for the model\n",
    "plt.plot(rf_train_fpr,rf_train_tpr, marker='.')\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B.2   Random Forest Classifier - TEST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_test_predict_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_test_predict_rf)\n",
    "sns.heatmap(confusion_matrix(y_test, y_test_predict_rf),annot=True, fmt='d',cbar=False, cmap=\"mako\")\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('Actual Label')\n",
    "plt.title('Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_test_acc=model_rf_reg.score(X_test,y_test)\n",
    "rf_test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_test_predict_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_metrics=classification_report(y_test,y_test_predict_rf,output_dict=True)\n",
    "df=pd.DataFrame(rf_metrics).transpose()\n",
    "rf_test_precision=round(df.loc[\"1\"][0],2)\n",
    "rf_test_recall=round(df.loc[\"1\"][1],2)\n",
    "rf_test_f1=round(df.loc[\"1\"][2],2)\n",
    "print ('rf_test_precision ',rf_test_precision)\n",
    "print ('rf_test_recall ',rf_test_recall)\n",
    "print ('rf_test_f1 ',rf_test_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict probabilities\n",
    "probs = model_rf_reg.predict_proba(X_test)\n",
    "\n",
    "# keep probabilities for the positive outcome only\n",
    "probs = probs[:, 1]\n",
    "\n",
    "# calculate AUC\n",
    "rf_test_auc = roc_auc_score(y_test, probs)\n",
    "print('AUC: %.3f' % rf_test_auc)\n",
    "\n",
    "# calculate roc curve\n",
    "rf_test_fpr,rf_test_tpr, thresholds = roc_curve(y_test, probs)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "\n",
    "# plot the roc curve for the model\n",
    "plt.plot(rf_test_fpr,rf_test_tpr, marker='.')\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C.  MLP CLASSIFIER (ARTIFICIAL NEURAL NETWORK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C.1  Artificial Neural Network - TRAIN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "confusion_matrix(y_train,y_train_predict_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_train,y_train_predict_mlp)\n",
    "sns.heatmap(confusion_matrix(y_train,y_train_predict_mlp),annot=True, fmt='d',cbar=False, cmap=\"mako\")\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('Actual Label')\n",
    "plt.title('Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_train_acc=clf.score(X_train_z,y_train) \n",
    "nn_train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_train,y_train_predict_mlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_metrics=classification_report(y_train, y_train_predict_mlp,output_dict=True)\n",
    "df=pd.DataFrame(nn_metrics).transpose()\n",
    "nn_train_precision=round(df.loc[\"1\"][0],2)\n",
    "nn_train_recall=round(df.loc[\"1\"][1],2)\n",
    "nn_train_f1=round(df.loc[\"1\"][2],2)\n",
    "print ('nn_train_precision ',nn_train_precision)\n",
    "print ('nn_train_recall ',nn_train_recall)\n",
    "print ('nn_train_f1 ',nn_train_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# predict probabilities\n",
    "probs = clf.predict_proba(X_train_z)\n",
    "\n",
    "# keep probabilities for the positive outcome only\n",
    "probs = probs[:, 1]\n",
    "\n",
    "# calculate AUC\n",
    "nn_train_auc = roc_auc_score(y_train, probs)\n",
    "print('AUC: %.3f' % nn_train_auc)\n",
    "\n",
    "# calculate roc curve\n",
    "nn_train_fpr,nn_train_tpr, thresholds = roc_curve(y_train, probs)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "\n",
    "# plot the roc curve for the model\n",
    "plt.plot(nn_train_fpr,nn_train_tpr, marker='.')\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C.2  Artificial Neural Network - TEST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "confusion_matrix(y_test,y_test_predict_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test,y_test_predict_mlp)\n",
    "sns.heatmap(confusion_matrix(y_test,y_test_predict_mlp),annot=True, fmt='d',cbar=False, cmap=\"mako\")\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('Actual Label')\n",
    "plt.title('Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_test_acc=clf.score(X_test_z,y_test)\n",
    "nn_test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_test_predict_mlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_metrics=classification_report(y_test, y_test_predict_mlp,output_dict=True)\n",
    "df=pd.DataFrame(nn_metrics).transpose()\n",
    "nn_test_precision=round(df.loc[\"1\"][0],2)\n",
    "nn_test_recall=round(df.loc[\"1\"][1],2)\n",
    "nn_test_f1=round(df.loc[\"1\"][2],2)\n",
    "print ('nn_test_precision ',nn_test_precision)\n",
    "print ('nn_test_recall ',nn_test_recall)\n",
    "print ('nn_test_f1 ',nn_test_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict probabilities\n",
    "probs = clf.predict_proba(X_test_z)\n",
    "\n",
    "# keep probabilities for the positive outcome only\n",
    "probs = probs[:, 1]\n",
    "\n",
    "# calculate AUC\n",
    "nn_test_auc = roc_auc_score(y_test, probs)\n",
    "print('AUC: %.3f' % nn_test_auc)\n",
    "\n",
    "# calculate roc curve\n",
    "nn_test_fpr,nn_test_tpr, thresholds = roc_curve(y_test, probs)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "\n",
    "# plot the roc curve for the model\n",
    "plt.plot(nn_test_fpr,nn_test_tpr, marker='.')\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Final Model: Compare all the model and write an inference which model is best/optimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=['Accuracy', 'AUC', 'Recall','Precision','F1 Score']\n",
    "data = pd.DataFrame({'CART Train':[cart_train_acc,cart_train_auc,cart_train_recall,cart_train_precision,cart_train_f1],\n",
    "        'CART Test':[cart_test_acc,cart_test_auc,cart_test_recall,cart_test_precision,cart_test_f1],\n",
    "       'Random Forest Train':[rf_train_acc,rf_train_auc,rf_train_recall,rf_train_precision,rf_train_f1],\n",
    "        'Random Forest Test':[rf_test_acc,rf_test_auc,rf_test_recall,rf_test_precision,rf_test_f1],\n",
    "       'Neural Network Train':[nn_train_acc,nn_train_auc,nn_train_recall,nn_train_precision,nn_train_f1],\n",
    "        'Neural Network Test':[nn_test_acc,nn_test_auc,nn_test_recall,nn_test_precision,nn_test_f1]},index=index)\n",
    "round(data,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.plot(cart_train_fpr, cart_train_tpr,color='red',label=\"CART\")\n",
    "plt.plot(rf_train_fpr,rf_train_tpr,color='green',label=\"RF\")\n",
    "plt.plot(nn_train_fpr,nn_train_tpr,color='black',label=\"NN\")\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Train')\n",
    "plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.plot(cart_test_fpr, cart_test_tpr,color='red',label=\"CART\")\n",
    "plt.plot(rf_test_fpr,rf_test_tpr,color='green',label=\"RF\")\n",
    "plt.plot(nn_test_fpr,nn_test_tpr,color='black',label=\"NN\")\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Test')\n",
    "plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "•\tComparing the Train and Test Dataset, the accuracy for all the three models under consideration is not showing much of a difference. Also, considering only accuracy i.e., how accurately does the model classify the data points may not give us a full picture of the solution. \n",
    "\n",
    "•\tLooking at the AUC Score, the obvious preference would be Random Forest.\n",
    "\n",
    "•\tFurther, from Precision, which helps us identify how many are really positive among the positives identified as positives by the model again Random Forest is preferred and is closely followed by Neural Network. This is important from an insurance company perspective whose is particularly facing a higher claim for the tour insurance. \n",
    "\n",
    "•\tIt is strenuous to compare two models if there is low recall and high precision or high recall and low precision value. For this purpose, we can use f1-Score to compare the models. The highest f1-Score is observed for Random Forest model which is followed by Decision Tree.\n",
    "\n",
    "In conclusion, all the models have performed more or less in a similar fashion when compared with respect to all the metrics considered. In this case, out of all, Random Forest has performed slightly better than the rest (with a small difference of +0.05).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5  Inference: Basis on these predictions, what are the business insights and recommendations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "•\tOf the three models, Random Forest has performed slightly better than the other two models: Decision Tree and Neural Network. \n",
    "\n",
    "•\tIt is of prime importance that we use build a model which is efficient in predicting the claim status for us to provide valuable insights to the business.\n",
    "\n",
    "•\tIf the model is a black-box technique wherein setting the hyper-parameters is crucial, it can prove to be difficult to rely on that particular model to make a business acumen.\n",
    "\n",
    "•\tHowever, in this particular case, all the three models have shown as a stable performance considering all the metrics of comparison.\n",
    "\n",
    "•\tOne useful metrics from Decision Tree and Random Forest is the variable importance which stands crucial for the business. From both the models, we saw that ‘Agency Code’ was the most important variable followed by ‘Sales’ and ‘Product Name’. These variables are potential identifiers for whether the insured claimed or not.\n",
    "\n",
    "•\tOn the other hand, the variables that are of least importance are: ‘Type’ and ‘Channel’."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
